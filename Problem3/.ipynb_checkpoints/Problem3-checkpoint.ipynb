{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Result:        o                  e  e  e     e  e     eo     o  e               e  e  e  e          e          e  e          e  e  e     e                e     e  e         o           e         e         e                         e    e  e     eo            e  e  e           o     o          e      e         eo e             e  e \n",
      "Loss value: \n",
      " 2.8206403255462646\n",
      "Output Result:  aer   nl sooe baviagthn snf Lortagetiof Lorem Ipsumeavenlaliarie thedmavormth hobe suffere  aleomesiln tn toms efrmn wh bojeme   hndsureardanised tormg torbg donrt loof eves tnightli baliogeblebff Lor mne toisg torim da oassage of Lorem Ipsumeav s d  to in mfre toi e inndt hns ning tiie tasiigg hinga  in toe maleye of Loreh\n",
      "Loss value: \n",
      " 1.7285345792770386\n",
      "Output Result:  oere mne many varietions of passaget of Lorem Ipsumyavailablebut the majority have suffered altoration in some form, by injected humourrandomised words which don't look even tlightly believable.If you are going to use a passage of Lorem Ipsumyou neer th be sure there isn't anything tmberrassing hidden in the mindle of textw\n",
      "Loss value: \n",
      " 0.9502033591270447\n",
      "Output Result:  huce are many variations of passages of Lorem Ipsum availablebut the majority have suffered alteration in some form, by injected humourrandomised words which don't look even slightly believable.If you are going to use a passage of Lorem Ipsum ou need to be sure there isn't anything embarrassing hidden in the middle of text.\n",
      "Loss value: \n",
      " 0.6719791293144226\n",
      "Output Result:  hure are many variations of passages of Lorem Ipsumyavailablebut the majority have suffered alteration in some form, by injected humourrandomised words which don't look even slightly believable.If you are going to use a passage of Lorem Ipsumyou need to be sure there isn't anything embarrassing hidden in the middle of text.\n",
      "Loss value: \n",
      " 0.5331106185913086\n",
      "Output Result:  here are many variations of passages of Lorem Ipsum availablebut the majority have suffered alteration in some form, by injected humourrandomised words which don't look even slightly believable.If you are going to use a passage of Lorem Ipsum ou need to be sure there isn't anything embarrassing hidden in the middle of text.\n",
      "Loss value: \n",
      " 0.4506951570510864\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[주의!]\n",
    "상단 코드는 수정하지 마세요\n",
    "\"\"\"\n",
    "###################################################################\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "############# 해당 부분을 구현하시면 됩니다#########################\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "###################### 구현 부분 끝  ##############################\n",
    "###################################################################\n",
    "\"\"\"\n",
    "[주의!]\n",
    "하단 코드는 수정하지 마세요\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##SEED값 고정\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    " \n",
    "\n",
    "\n",
    "sentence = (\"There are many variations of passages of Lorem Ipsum available\"\n",
    "            \"but the majority have suffered alteration in some form, by injected humour\"\n",
    "            \"randomised words which don't look even slightly believable.\"\n",
    "            \"If you are going to use a passage of Lorem Ipsum\"\n",
    "            \"you need to be sure there isn't anything embarrassing hidden in the middle of text.\")\n",
    "\n",
    "# make dictionary\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}\n",
    "\n",
    "# hyper parameters\n",
    "dic_size = len(char_dic)\n",
    "hidden_size = len(char_dic)\n",
    "sequence_length = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# data setting\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str])  # y str to index\n",
    "\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "\n",
    "# transform as torch tensor variable\n",
    "X = torch.FloatTensor(x_one_hot).to(DEVICE)\n",
    "Y = torch.LongTensor(y_data).to(DEVICE)\n",
    "\n",
    "model = MyModel(dic_size, hidden_size).to(DEVICE)\n",
    "\n",
    "# loss & optimizer setting\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "\n",
    "# start training\n",
    "for i in range(1, 601):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X).to(DEVICE)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0:\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else:\n",
    "            predict_str += char_set[result[-1]]\n",
    "    if i % 100 == 0:\n",
    "        print(\"Output Result: \", predict_str)\n",
    "        print(\"Loss value: \\n\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lee",
   "language": "python",
   "name": "lee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
